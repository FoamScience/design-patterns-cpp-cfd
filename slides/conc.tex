%\section*{Advanced topics}

\begin{frame}{Advanced topics}

    The need for Load Balancing in AMR settings

    \begin{itemize}
        \item AMR operations tend to unbalance cell count distribution accross processors
        \item Using Blocking comms means more idle process time
            \begin{itemize}
                \item Non-Blocking are not a solution.
                \item Spending some time on rebalancing the mesh is.
            \end{itemize}
        \item Naturally, load balancing itself involves parallel communication!
    \end{itemize}
\end{frame}

\begin{frame}{Licensing}

    This work is licensed under a
    \href{http://creativecommons.org/licenses/by-sa/4.0/}{Creative Commons
    Attribution-ShareAlike 4.0 International License}.

    Code snippets are licensed under a \href{http://www.gnu.org/licenses/gpl.txt}{GNU Public License}.

    \begin{center}\ccbysa \hspace{2mm} \textsuperscript{2022}\end{center}

\end{frame}

\begin{frame}[standout]
  Questions?
\end{frame}

\appendix

\begin{frame}[fragile]{Compile and link against MPI implementations}
Compiler wrappers are your best friends!
\begin{CodeEnv}[Grab correct compiler/linker flags]{bash}{\tiny-I/usr/lib/x86\_64-linux-gnu/openmpi/include/openmpi ...\\-pthread -L/usr/lib/x86\_64-linux-gnu/openmpi/lib ...}{\scriptsize}
mpic++ --showme:compiler
mpic++ --showme:linker
\end{CodeEnv}

OpenFOAM environment autmatically figures things out for you:
\begin{CodeEnvNoComment}[Typical Make/options file for the ESI fork]{bash}{\scriptsize}
include $(GENERAL_RULES)/mpi-rules
EXE_INC = $(PFLAGS) $(PINC) ...
LIB_LIBS = $(PLIBS) ...
\end{CodeEnvNoComment}
\end{frame}

\begin{frame}[fragile]{Miscellaneous}
    \begin{itemize}
        \item MPI standards: Blocking send can be used with a Non-blocking receive, and vice-versa
        \item But OpenFOAM wrapping makes it "non-trivial" to get it to work
    \end{itemize}
    \begin{itemize}
        \item You can still use MPI API directly, eg.  if you need one-sided communication.
    \end{itemize}
    \begin{itemize}
        \item Overlapping computation and communication for non-blocking calls is implemented on the MPI side, so,
        put your computations after the recieve call.
    \end{itemize}
\end{frame}


\begin{frame}[allowframebreaks]{Sources and further reading}
\printbibliography[heading=none]
\end{frame}
